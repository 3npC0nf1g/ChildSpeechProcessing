{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "fa03c202-d34e-4494-b994-6563105b9c81",
   "cell_type": "markdown",
   "source": [
    "## Installation "
   ],
   "metadata": {}
  },
  {
   "id": "572ee230-4466-4774-bd8e-19f9dc4648de",
   "cell_type": "code",
   "source": [
    "!pip install --upgrade --quiet pip\n",
    "!pip install --upgrade --quiet datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio huggingface_hub"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "9c666a42-6a82-4bdd-aabb-0aea9e9a3224",
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "b4fac9e2-a52e-4f28-abc0-37cf1fd8bc0a",
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {}
  },
  {
   "id": "5cb3da87-2e27-485c-9ded-6f2c13960fa8",
   "cell_type": "markdown",
   "source": [
    "### loader"
   ],
   "metadata": {}
  },
  {
   "id": "fcbdb9ff-4131-4039-a4dc-359d1a93f427",
   "cell_type": "code",
   "source": [
    "import os\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "\n",
    "def load_asr_split(folder_path):\n",
    "    data = {\"audio\": [], \"transcript\": []}\n",
    "\n",
    "    for file in sorted(os.listdir(folder_path)):\n",
    "        if file.endswith(\".wav\"):\n",
    "            txt = file.replace(\".wav\", \".txt\")\n",
    "            txt_path = os.path.join(folder_path, txt)\n",
    "\n",
    "            if os.path.exists(txt_path):\n",
    "                with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read().strip()\n",
    "\n",
    "                data[\"audio\"].append(os.path.join(folder_path, file))\n",
    "                data[\"transcript\"].append(text)\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "2e1b3d0e-bded-4915-ae7e-5b39837ee3e8",
   "cell_type": "code",
   "source": [
    "children_voice = DatasetDict({\n",
    "    \"train\": load_asr_split(\"/kaggle/input/childasr/train\"),\n",
    "    \"test\": load_asr_split(\"/kaggle/input/childasr/test\"),\n",
    "})\n",
    "\n",
    "children_voice\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "3ca2108c-562d-4a5c-9d02-a77890365087",
   "cell_type": "markdown",
   "source": [
    "## Load WhisperFeatureExtractor"
   ],
   "metadata": {}
  },
  {
   "id": "46ec1cc7-9562-42c9-8f40-32ff25d54a54",
   "cell_type": "code",
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-tiny\",\n",
    "    language=\"English\",\n",
    "    task=\"transcribe\"\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "4fcab715-41f9-4736-a5da-fef0d41ab12b",
   "cell_type": "markdown",
   "source": [
    "## Load WhisperTokenizer"
   ],
   "metadata": {}
  },
  {
   "id": "d3f41cde-0ce2-495a-b63f-912f22de3a39",
   "cell_type": "code",
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"English\", task=\"transcribe\")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "58c321cf-af4d-43a5-b034-00ccd00e847a",
   "cell_type": "markdown",
   "source": [
    "## WhisperProcessor"
   ],
   "metadata": {}
  },
  {
   "id": "aa46568b-50f0-4fc1-93ea-cb1546a55cc1",
   "cell_type": "code",
   "source": [
    "from transfomer import WhisperProcessor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"English\", task=\"transcribe\")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "c7e42daf-d2c8-4cd5-8128-01d646cfd142",
   "cell_type": "markdown",
   "source": [
    "### Prepare Data"
   ],
   "metadata": {}
  },
  {
   "id": "4e3be3d7-802f-41b1-a129-a2818eb794d5",
   "cell_type": "code",
   "source": [
    "children_voice[\"train\"][0]"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "cd31ecb4-19ff-458c-b315-c705335b1677",
   "cell_type": "code",
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    batch[\"input_features\"] = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcript\"]).input_ids\n",
    "    return batch"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "aa85decc-fc72-41cb-b936-05735ede0d61",
   "cell_type": "code",
   "source": [
    "children_voice = children_voice.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=children_voice[\"train\"].column_names,\n",
    "    num_proc=2\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "2402af2b-0e85-4841-a79c-4f35f09c9b56",
   "cell_type": "markdown",
   "source": [
    "# Training and Evaluation"
   ],
   "metadata": {}
  },
  {
   "id": "25bb6adb-2167-4f82-b24c-c8966d67d0b5",
   "cell_type": "markdown",
   "source": [
    "### Load Pre-Trained Checkpoint"
   ],
   "metadata": {}
  },
  {
   "id": "8303756b-5e62-4cb9-899d-40f3bc07353e",
   "cell_type": "code",
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "model.generation_config.language = \"English\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.forced_decoder_ids = None\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "bacdaab0-776b-407d-92b7-0b5fb71c2434",
   "cell_type": "markdown",
   "source": [
    "### Define a Data Collator"
   ],
   "metadata": {}
  },
  {
   "id": "577c9bf4-7c29-4646-a8a6-42770e7dcfac",
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features):\n",
    "        inputs = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(inputs, return_tensors=\"pt\")\n",
    "\n",
    "        labels = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(labels, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "d7c0eceb-1228-48ff-82e4-ad8e5e221933",
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "c4a268f3-1bef-473c-af88-ea3e46b58a16",
   "cell_type": "markdown",
   "source": [
    "### Evaluation Metrics"
   ],
   "metadata": {}
  },
  {
   "id": "6857562f-d5ae-472f-b164-613adf6d776c",
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    return {\"wer\": 100 * metric.compute(predictions=pred_str, references=label_str)}\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "5940c1bc-f341-492a-a3e0-28d0a0774726",
   "cell_type": "markdown",
   "source": [
    "### Define the Training Configuration"
   ],
   "metadata": {}
  },
  {
   "id": "675340f3-2894-42e7-92ed-d6323b00de3c",
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-tiny-en\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    max_steps=2000,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    push_to_hub=True,\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "1bce9355-54bd-441e-a444-2e39287e2b45",
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=children_voice[\"train\"],\n",
    "    eval_dataset=children_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")\n"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "c19b7b76-a584-4f15-8536-f6bc6f3017bb",
   "cell_type": "code",
   "source": [
    "processor.save_pretrained(training_args.output_dir)"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "ce53800d-b822-4685-aae5-d081f8b13f3c",
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {}
  },
  {
   "id": "534965f1-4f7e-4cd9-914a-d4d1adb8f426",
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "id": "cf65c479-eb6f-4149-9354-515e066653bc",
   "cell_type": "markdown",
   "source": [
    "## Building a Demo"
   ],
   "metadata": {}
  },
  {
   "id": "8d46755f-7822-4edd-bcb2-249d0b0e791b",
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=training_args.output_dir\n",
    ")\n",
    "\n",
    "def transcribe(audio):\n",
    "    return pipe(audio)[\"text\"]\n",
    "\n",
    "gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Tiny (Fine-tuned)\",\n",
    ").launch()"
   ],
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
