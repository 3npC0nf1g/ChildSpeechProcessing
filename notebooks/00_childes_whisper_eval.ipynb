{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTEBOOK: Whisper Evaluation & Dataset Creation\n",
    "√âtape 3: Calculer le WER avec Whisper baseline\n",
    "√âtape 4: Filtrer les voix d'enfants\n",
    "√âtape 5: Cr√©er le dataset d'entra√Ænement\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from extract_wor import WorSegment\n",
    "from children_voice_filter import ChildrenVoiceFilter, SegmentFilter\n",
    "import whisper\n",
    "from jiwer import wer as compute_wer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 1: WER CALCULATION - Calculer le WER avec Whisper baseline\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class TranscriptionResult:\n",
    "    \"\"\"R√©sultat de transcription d'un segment\"\"\"\n",
    "    segment_id: str\n",
    "    speaker: str\n",
    "    file_name: str\n",
    "    audio_path: str\n",
    "    ground_truth: str\n",
    "    whisper_prediction: str\n",
    "    duration_ms: int\n",
    "    wer: float\n",
    "    confidence: float = 0.0\n",
    "\n",
    "\n",
    "class WhisperBaseline:\n",
    "    \"\"\"Transcrire avec Whisper et calculer le WER\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"base\"):\n",
    "        \"\"\"\n",
    "        Initialiser Whisper\n",
    "        \n",
    "        Args:\n",
    "            model_name: Mod√®le √† utiliser (tiny, base, small, medium, large)\n",
    "        \"\"\"\n",
    "        print(f\"üì¶ Chargement Whisper '{model_name}'...\")\n",
    "        self.model = whisper.load_model(model_name)\n",
    "        print(\"‚úì Mod√®le charg√©\\n\")\n",
    "    \n",
    "    def transcribe_segment(self, audio_path: Path, language: str = \"fr\") -> Dict:\n",
    "        \"\"\"\n",
    "        Transcrire un fichier audio\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Chemin du fichier audio\n",
    "            language: Code langue (fr, en, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            Dict avec 'text' et 'confidence'\n",
    "        \"\"\"\n",
    "        if not audio_path.exists():\n",
    "            return {\"text\": \"\", \"confidence\": 0.0}\n",
    "        \n",
    "        try:\n",
    "            result = self.model.transcribe(\n",
    "                str(audio_path),\n",
    "                language=language,\n",
    "                verbose=False\n",
    "            )\n",
    "            return {\n",
    "                \"text\": result[\"text\"].strip(),\n",
    "                \"confidence\": result.get(\"confidence\", 0.0)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Erreur: {audio_path.name} - {e}\")\n",
    "            return {\"text\": \"\", \"confidence\": 0.0}\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_wer(ground_truth: str, prediction: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculer le Word Error Rate\n",
    "        \n",
    "        Args:\n",
    "            ground_truth: Texte de r√©f√©rence\n",
    "            prediction: Texte pr√©dit par Whisper\n",
    "        \n",
    "        Returns:\n",
    "            WER entre 0.0 (parfait) et 1.0+ (tr√®s mauvais)\n",
    "        \"\"\"\n",
    "        if not ground_truth.strip():\n",
    "            return 0.0 if not prediction.strip() else 1.0\n",
    "        \n",
    "        return compute_wer(ground_truth, prediction)\n",
    "\n",
    "\n",
    "class BaselineEvaluator:\n",
    "    \"\"\"√âvaluer Whisper baseline sur les segments audio\"\"\"\n",
    "    \n",
    "    def __init__(self, whisper_model: str = \"base\"):\n",
    "        self.whisper = WhisperBaseline(whisper_model)\n",
    "    \n",
    "    def evaluate(self, audio_segments: List[Dict], sample_size: int = None, \n",
    "                batch_size: int = 50) -> List[TranscriptionResult]:\n",
    "        \"\"\"\n",
    "        √âvaluer Whisper sur tous les segments audio\n",
    "        \n",
    "        Args:\n",
    "            audio_segments: R√©sultats de AudioSegmenter.segment_all()\n",
    "            sample_size: √âvaluer seulement N segments (None = tous)\n",
    "            batch_size: Afficher progress tous les N segments\n",
    "        \n",
    "        Returns:\n",
    "            Liste de TranscriptionResult\n",
    "        \"\"\"\n",
    "        \n",
    "        if sample_size:\n",
    "            audio_segments = audio_segments[:sample_size]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        print(f\"üé§ √âvaluation Whisper baseline ({len(audio_segments)} segments)\\n\")\n",
    "        \n",
    "        for i, segment in enumerate(audio_segments):\n",
    "            audio_path = Path(segment[\"audio_path\"])\n",
    "            \n",
    "            # Transcrire\n",
    "            transcription = self.whisper.transcribe_segment(audio_path)\n",
    "            \n",
    "            # Calculer WER\n",
    "            wer = self.whisper.calculate_wer(\n",
    "                segment[\"text\"],\n",
    "                transcription[\"text\"]\n",
    "            )\n",
    "            \n",
    "            # Cr√©er r√©sultat\n",
    "            result = TranscriptionResult(\n",
    "                segment_id=segment[\"segment_id\"],\n",
    "                speaker=segment[\"speaker\"],\n",
    "                file_name=segment[\"file_name\"],\n",
    "                audio_path=segment[\"audio_path\"],\n",
    "                ground_truth=segment[\"text\"],\n",
    "                whisper_prediction=transcription[\"text\"],\n",
    "                duration_ms=segment[\"duration_ms\"],\n",
    "                wer=wer,\n",
    "                confidence=transcription[\"confidence\"]\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            # Progress\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                avg_wer = sum(r.wer for r in results) / len(results)\n",
    "                print(f\"  ‚úì {i + 1}/{len(audio_segments)} | Avg WER: {avg_wer:.3f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def print_wer_statistics(results: List[TranscriptionResult]):\n",
    "    \"\"\"Afficher les statistiques WER\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå Aucun r√©sultat\")\n",
    "        return\n",
    "    \n",
    "    wers = [r.wer for r in results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä WER STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìà Globales:\")\n",
    "    print(f\"   Total segments:    {len(results)}\")\n",
    "    print(f\"   WER moyen:         {sum(wers) / len(wers):.3f}\")\n",
    "    print(f\"   WER min:           {min(wers):.3f}\")\n",
    "    print(f\"   WER max:           {max(wers):.3f}\")\n",
    "    print(f\"   WER median:        {sorted(wers)[len(wers)//2]:.3f}\")\n",
    "    \n",
    "    # Par range\n",
    "    print(f\"\\n   Distribution:\")\n",
    "    ranges = [(0.0, 0.1), (0.1, 0.3), (0.3, 0.5), (0.5, 1.0)]\n",
    "    for low, high in ranges:\n",
    "        count = sum(1 for w in wers if low <= w < high)\n",
    "        pct = (count / len(wers)) * 100\n",
    "        print(f\"      {low:.1f}-{high:.1f}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    # Par speaker\n",
    "    by_speaker = {}\n",
    "    for r in results:\n",
    "        by_speaker.setdefault(r.speaker, []).append(r.wer)\n",
    "    \n",
    "    print(f\"\\nüë• Par speaker:\")\n",
    "    for speaker in sorted(by_speaker.keys()):\n",
    "        wers_speaker = by_speaker[speaker]\n",
    "        avg = sum(wers_speaker) / len(wers_speaker)\n",
    "        print(f\"      {speaker:15} {len(wers_speaker):4d} segments | WER: {avg:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 2: CHILDREN FILTERING - Filtrer pour garder uniquement les enfants\n",
    "# ============================================================================\n",
    "\n",
    "def filter_children_results(results: List[TranscriptionResult], \n",
    "                           speakers_info: Dict[str, str]) -> List[TranscriptionResult]:\n",
    "    \"\"\"\n",
    "    Filtrer les r√©sultats pour ne garder que les enfants\n",
    "    \n",
    "    Args:\n",
    "        results: R√©sultats de BaselineEvaluator.evaluate()\n",
    "        speakers_info: Dict {speaker_name: role} du .cha\n",
    "    \n",
    "    Returns:\n",
    "        R√©sultats filtr√©s aux enfants seulement\n",
    "    \"\"\"\n",
    "    \n",
    "    children = []\n",
    "    adults = []\n",
    "    unknown = []\n",
    "    \n",
    "    for r in results:\n",
    "        role = speakers_info.get(r.speaker, \"Unknown\")\n",
    "        \n",
    "        if role in ChildrenVoiceFilter.CHILD_ROLES:\n",
    "            children.append(r)\n",
    "        elif role in ChildrenVoiceFilter.ADULT_ROLES:\n",
    "            adults.append(r)\n",
    "        else:\n",
    "            unknown.append(r)\n",
    "    \n",
    "    return children, adults, unknown\n",
    "\n",
    "\n",
    "def print_filtering_report(children: List[TranscriptionResult],\n",
    "                          adults: List[TranscriptionResult],\n",
    "                          unknown: List[TranscriptionResult]):\n",
    "    \"\"\"Afficher le rapport de filtrage\"\"\"\n",
    "    \n",
    "    total = len(children) + len(adults) + len(unknown)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üßí CHILDREN FILTERING REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìä Distribution:\")\n",
    "    print(f\"   Total segments:        {total}\")\n",
    "    print(f\"   ‚úÖ Enfants:            {len(children)} ({len(children)/total*100:.1f}%)\")\n",
    "    print(f\"   ‚ùå Adultes:            {len(adults)} ({len(adults)/total*100:.1f}%)\")\n",
    "    print(f\"   ‚ùì R√¥le inconnu:       {len(unknown)} ({len(unknown)/total*100:.1f}%)\")\n",
    "    \n",
    "    if children:\n",
    "        wers_children = [r.wer for r in children]\n",
    "        print(f\"\\nüë®‚Äçüë©‚Äçüëß Enfants:\")\n",
    "        print(f\"   Segments:              {len(children)}\")\n",
    "        print(f\"   Dur√©e audio:           {sum(r.duration_ms for r in children) / 1000 / 60:.1f} minutes\")\n",
    "        print(f\"   WER moyen:             {sum(wers_children) / len(wers_children):.3f}\")\n",
    "        \n",
    "        # Par speaker enfant\n",
    "        by_speaker = {}\n",
    "        for r in children:\n",
    "            by_speaker.setdefault(r.speaker, []).append(r)\n",
    "        \n",
    "        print(f\"\\n   Par speaker ({len(by_speaker)}):\")\n",
    "        for speaker in sorted(by_speaker.keys()):\n",
    "            segs = by_speaker[speaker]\n",
    "            wer_avg = sum(r.wer for r in segs) / len(segs)\n",
    "            duration = sum(r.duration_ms for r in segs) / 1000\n",
    "            print(f\"      {speaker:15} {len(segs):4d} segments | {duration:7.1f}s | WER: {wer_avg:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 3: DATASET CREATION - Cr√©er le dataset pour fine-tuning\n",
    "# ============================================================================\n",
    "\n",
    "class TrainingDatasetBuilder:\n",
    "    \"\"\"Cr√©er les splits train/test pour fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Path):\n",
    "        \"\"\"\n",
    "        Initialiser le builder\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Dossier o√π sauvegarder les datasets\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def create_splits(self, results: List[TranscriptionResult], \n",
    "                     train_ratio: float = 0.8,\n",
    "                     min_wer: float = None,\n",
    "                     max_wer: float = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Cr√©er les splits train/test\n",
    "        \n",
    "        Args:\n",
    "            results: R√©sultats filtr√©s (enfants seulement)\n",
    "            train_ratio: Ratio train/test (0.8 = 80% train, 20% test)\n",
    "            min_wer: WER minimum √† accepter (None = pas de limite)\n",
    "            max_wer: WER maximum √† accepter (None = pas de limite)\n",
    "        \n",
    "        Returns:\n",
    "            Dict avec train/test datasets\n",
    "        \"\"\"\n",
    "        \n",
    "        # Filtrer par WER si sp√©cifi√©\n",
    "        filtered = results\n",
    "        if min_wer is not None or max_wer is not None:\n",
    "            filtered = [\n",
    "                r for r in results\n",
    "                if (min_wer is None or r.wer >= min_wer) and\n",
    "                   (max_wer is None or r.wer <= max_wer)\n",
    "            ]\n",
    "        \n",
    "        print(f\"üìä Creating splits from {len(filtered)} segments\")\n",
    "        print(f\"   WER range: [{min_wer or '0'}, {max_wer or 'inf'}]\")\n",
    "        \n",
    "        # Split\n",
    "        split_idx = int(len(filtered) * train_ratio)\n",
    "        train = filtered[:split_idx]\n",
    "        test = filtered[split_idx:]\n",
    "        \n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"test\": test,\n",
    "            \"total\": len(filtered)\n",
    "        }\n",
    "    \n",
    "    def save_jsonl(self, results: List[TranscriptionResult], output_file: Path, \n",
    "                  data_type: str = \"training\"):\n",
    "        \"\"\"\n",
    "        Sauvegarder en format JSONL pour Whisper\n",
    "        \n",
    "        Args:\n",
    "            results: TranscriptionResult √† sauvegarder\n",
    "            output_file: Chemin du fichier JSONL\n",
    "            data_type: \"training\" ou \"evaluation\"\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for r in results:\n",
    "                entry = {\n",
    "                    \"audio\": r.audio_path,\n",
    "                    \"text\": r.ground_truth,\n",
    "                    \"language\": \"fr\"\n",
    "                }\n",
    "                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "        \n",
    "        print(f\"‚úÖ {data_type}: {len(results)} segments ‚Üí {output_file.name}\")\n",
    "    \n",
    "    def save_metadata_json(self, results: List[TranscriptionResult], output_file: Path):\n",
    "        \"\"\"\n",
    "        Sauvegarder les m√©tadonn√©es compl√®tes en JSON\n",
    "        \n",
    "        Args:\n",
    "            results: TranscriptionResult √† sauvegarder\n",
    "            output_file: Chemin du fichier JSON\n",
    "        \"\"\"\n",
    "        \n",
    "        data = [\n",
    "            {\n",
    "                \"segment_id\": r.segment_id,\n",
    "                \"speaker\": r.speaker,\n",
    "                \"file_name\": r.file_name,\n",
    "                \"audio_path\": r.audio_path,\n",
    "                \"ground_truth\": r.ground_truth,\n",
    "                \"whisper_prediction\": r.whisper_prediction,\n",
    "                \"duration_ms\": r.duration_ms,\n",
    "                \"wer\": r.wer,\n",
    "                \"confidence\": r.confidence\n",
    "            }\n",
    "            for r in results\n",
    "        ]\n",
    "        \n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Metadata: {len(results)} segments ‚Üí {output_file.name}\")\n",
    "    \n",
    "    def create_complete_dataset(self, results: List[TranscriptionResult],\n",
    "                               train_ratio: float = 0.8,\n",
    "                               min_wer: float = None,\n",
    "                               max_wer: float = None):\n",
    "        \"\"\"\n",
    "        Cr√©er le dataset complet: train/test en JSONL + metadata JSON\n",
    "        \n",
    "        Args:\n",
    "            results: R√©sultats filtr√©s (enfants seulement)\n",
    "            train_ratio: Ratio train/test\n",
    "            min_wer: WER minimum\n",
    "            max_wer: WER maximum\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üì¶ CREATING TRAINING DATASET\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Cr√©er splits\n",
    "        splits = self.create_splits(results, train_ratio, min_wer, max_wer)\n",
    "        \n",
    "        print(f\"   Train: {len(splits['train'])} segments\")\n",
    "        print(f\"   Test:  {len(splits['test'])} segments\\n\")\n",
    "        \n",
    "        # Sauvegarder en JSONL (pour fine-tuning)\n",
    "        self.save_jsonl(splits[\"train\"], self.output_dir / \"train.jsonl\", \"Training\")\n",
    "        self.save_jsonl(splits[\"test\"], self.output_dir / \"eval.jsonl\", \"Evaluation\")\n",
    "        \n",
    "        # Sauvegarder metadata (pour analyse)\n",
    "        self.save_metadata_json(splits[\"train\"], self.output_dir / \"train_metadata.json\")\n",
    "        self.save_metadata_json(splits[\"test\"], self.output_dir / \"eval_metadata.json\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"‚úÖ DATASET CREATED\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nüìÅ Fichiers g√©n√©r√©s:\")\n",
    "        print(f\"   train.jsonl ................. {len(splits['train'])} segments\")\n",
    "        print(f\"   eval.jsonl .................. {len(splits['test'])} segments\")\n",
    "        print(f\"   train_metadata.json ......... Stats compl√®tes (train)\")\n",
    "        print(f\"   eval_metadata.json .......... Stats compl√®tes (eval)\")\n",
    "        print(f\"\\nüìÇ Emplacement: {self.output_dir}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 4: PIPELINE ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_whisper_dataset_pipeline(audio_segments: List[Dict],\n",
    "                                   matched_pairs: List[Tuple[Path, Path]],\n",
    "                                   output_dir: Path,\n",
    "                                   whisper_model: str = \"base\",\n",
    "                                   sample_size: int = None,\n",
    "                                   train_ratio: float = 0.8):\n",
    "    \"\"\"\n",
    "    Pipeline complet: Whisper ‚Üí WER ‚Üí Filter ‚Üí Dataset\n",
    "    \n",
    "    Args:\n",
    "        audio_segments: R√©sultats de AudioSegmenter.segment_all()\n",
    "        matched_pairs: Liste de paires (cha_file, audio_file)\n",
    "        output_dir: Dossier de sortie\n",
    "        whisper_model: Mod√®le Whisper √† utiliser\n",
    "        sample_size: √âvaluer N segments seulement (pour test)\n",
    "        train_ratio: Ratio train/test\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ WHISPER EVALUATION & DATASET PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # √âTAPE 1: √âvaluation Whisper\n",
    "    print(\"\\n1Ô∏è‚É£  STEP 1: Evaluate Whisper baseline\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    evaluator = BaselineEvaluator(whisper_model=whisper_model)\n",
    "    results = evaluator.evaluate(audio_segments, sample_size=sample_size, batch_size=50)\n",
    "    \n",
    "    print_wer_statistics(results)\n",
    "    \n",
    "    # √âTAPE 2: Filtrer enfants\n",
    "    print(\"2Ô∏è‚É£  STEP 2: Filter children voices\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Extraire les m√©tadonn√©es speakers de tous les .cha\n",
    "    all_speakers_info = {}\n",
    "    for cha_file, _ in matched_pairs:\n",
    "        speakers_info = ChildrenVoiceFilter.extract_speakers_from_cha(cha_file)\n",
    "        all_speakers_info.update(speakers_info)\n",
    "    \n",
    "    children, adults, unknown = filter_children_results(results, all_speakers_info)\n",
    "    \n",
    "    print_filtering_report(children, adults, unknown)\n",
    "    \n",
    "    # √âTAPE 3: Cr√©er dataset\n",
    "    print(\"3Ô∏è‚É£  STEP 3: Create training dataset\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    builder = TrainingDatasetBuilder(output_dir / \"training_dataset\")\n",
    "    builder.create_complete_dataset(\n",
    "        children,\n",
    "        train_ratio=train_ratio,\n",
    "        min_wer=None,  # Inclure tous les enfants\n",
    "        max_wer=None\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"all_results\": results,\n",
    "        \"children\": children,\n",
    "        \"adults\": adults,\n",
    "        \"unknown\": unknown\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN: Utiliser le pipeline\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    output_dir = Path(\"output/whisper_evaluation\")\n",
    "    \n",
    "    # Exemple: tu dois avoir les audio_segments du AudioSegmenter\n",
    "    # Ici on montre juste la structure\n",
    "    \n",
    "    print(\"üìù Exemple d'utilisation:\")\n",
    "    print(\"\\nfrom data_processing_notebook import *\")\n",
    "    print(\"from whisper_evaluation_dataset import *\")\n",
    "    print(\"\\n# 1. Processing\")\n",
    "    print(\"pipeline = DataProcessingPipeline(cha_dir, audio_dir, output_dir)\")\n",
    "    print(\"results = pipeline.run()\")\n",
    "    print(\"\\n# 2. Whisper + Dataset\")\n",
    "    print(\"create_whisper_dataset_pipeline(\")\n",
    "    print(\"    audio_segments=results['extracted'],\")\n",
    "    print(\"    matched_pairs=...,\")\n",
    "    print(\"    output_dir=output_dir,\")\n",
    "    print(\"    sample_size=500  # Optionnel: test sur 500 segments\")\n",
    "    print(\")\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6599737c144c444"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
