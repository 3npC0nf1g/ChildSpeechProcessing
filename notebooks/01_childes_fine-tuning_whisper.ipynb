{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WHISPER FINE-TUNING NOTEBOOK\n",
    "Input: train.jsonl + eval.jsonl from data processing pipeline\n",
    "Output: Fine-tuned Whisper model + WER comparison (baseline vs fine-tuned)\n",
    "\n",
    "Requirements:\n",
    "pip install transformers datasets evaluate jiwer librosa soundfile torch accelerate\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from jiwer import wer as compute_wer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    \"dataset_dir\": Path(\"output/whisper_children_dataset/training_dataset\"),\n",
    "    \"model_name\": \"openai/whisper-base\",\n",
    "    \"output_dir\": Path(\"output/whisper_finetuned\"),\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"logging_steps\": 100,\n",
    "    \"save_steps\": 500,\n",
    "    \"eval_steps\": 500,\n",
    "    \"language\": \"French\",\n",
    "    \"task\": \"transcribe\"\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 1: LOAD DATASETS FROM JSONL\n",
    "# ============================================================================\n",
    "\n",
    "def load_datasets_from_jsonl(dataset_dir: Path) -> DatasetDict:\n",
    "    \"\"\"Load train and eval datasets from JSONL files\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“‚ STEP 1: LOAD DATASETS FROM JSONL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    train_file = dataset_dir / \"train.jsonl\"\n",
    "    eval_file = dataset_dir / \"eval.jsonl\"\n",
    "    \n",
    "    if not train_file.exists() or not eval_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing JSONL files in {dataset_dir}\")\n",
    "    \n",
    "    # Load JSONL files\n",
    "    def load_jsonl(path):\n",
    "        data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        return data\n",
    "    \n",
    "    train_data = load_jsonl(train_file)\n",
    "    eval_data = load_jsonl(eval_file)\n",
    "    \n",
    "    print(f\"\\nâœ“ Loaded {len(train_data)} training samples\")\n",
    "    print(f\"âœ“ Loaded {len(eval_data)} evaluation samples\")\n",
    "    \n",
    "    # Create Hugging Face Datasets\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"audio\": [d[\"audio\"] for d in train_data],\n",
    "        \"text\": [d[\"text\"] for d in train_data],\n",
    "    })\n",
    "    \n",
    "    eval_dataset = Dataset.from_dict({\n",
    "        \"audio\": [d[\"audio\"] for d in eval_data],\n",
    "        \"text\": [d[\"text\"] for d in eval_data],\n",
    "    })\n",
    "    \n",
    "    # Cast to Audio type (Hugging Face will load audio files)\n",
    "    train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    eval_dataset = eval_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    \n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"eval\": eval_dataset\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ… Datasets created with audio loading\\n\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 2: PREPARE DATA FOR TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "def prepare_dataset(batch, processor, language=\"French\", task=\"transcribe\"):\n",
    "    \"\"\"Prepare audio and text for training\"\"\"\n",
    "    \n",
    "    # Load and process audio\n",
    "    audio = batch[\"audio\"]\n",
    "    \n",
    "    # Whisper expects audio to be resampled to 16kHz (already done by Audio())\n",
    "    # Extract mel spectrogram\n",
    "    inputs = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    # Tokenize text\n",
    "    with processor.as_target_processor():\n",
    "        labels = processor(batch[\"text\"], language=language)\n",
    "    \n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    batch[\"labels\"] = labels.input_ids\n",
    "    \n",
    "    return batch\n",
    "\n",
    "\n",
    "def prepare_datasets(datasets: DatasetDict, processor, config: Dict) -> DatasetDict:\n",
    "    \"\"\"Prepare all datasets for training\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ”§ STEP 2: PREPARE DATASETS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n  Preparing training dataset...\", end=\" \")\n",
    "    train_dataset = datasets[\"train\"].map(\n",
    "        lambda batch: prepare_dataset(batch, processor, config[\"language\"], config[\"task\"]),\n",
    "        remove_columns=[\"audio\", \"text\"],\n",
    "        num_proc=4\n",
    "    )\n",
    "    print(\"âœ“\")\n",
    "    \n",
    "    print(\"  Preparing evaluation dataset...\", end=\" \")\n",
    "    eval_dataset = datasets[\"eval\"].map(\n",
    "        lambda batch: prepare_dataset(batch, processor, config[\"language\"], config[\"task\"]),\n",
    "        remove_columns=[\"audio\", \"text\"],\n",
    "        num_proc=4\n",
    "    )\n",
    "    print(\"âœ“\")\n",
    "    \n",
    "    prepared = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"eval\": eval_dataset\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ… Datasets prepared\\n\")\n",
    "    \n",
    "    return prepared\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 3: FINE-TUNING\n",
    "# ============================================================================\n",
    "\n",
    "def finetune_whisper(prepared_datasets: DatasetDict, processor, config: Dict) -> WhisperForConditionalGeneration:\n",
    "    \"\"\"Fine-tune Whisper model\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸŽ“ STEP 3: FINE-TUNE WHISPER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"\\n  Loading model: {config['model_name']}...\", end=\" \")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(config[\"model_name\"])\n",
    "    print(\"âœ“\")\n",
    "    \n",
    "    # Freeze encoder (optional, speeds up training)\n",
    "    model.encoder.requires_grad_(False)\n",
    "    print(\"  Encoder frozen (optional)\")\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(config[\"output_dir\"]),\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        logging_steps=config[\"logging_steps\"],\n",
    "        save_steps=config[\"save_steps\"],\n",
    "        eval_steps=config[\"eval_steps\"],\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=prepared_datasets[\"train\"],\n",
    "        eval_dataset=prepared_datasets[\"eval\"],\n",
    "        data_collator=DataCollatorSpeechSeq2SeqWithPadding(processor),\n",
    "        compute_metrics=lambda x: compute_metrics(x, processor),\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸš€ Starting training...\\n\")\n",
    "    \n",
    "    # Fine-tune\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\nâœ… Fine-tuning completed\\n\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"Data collator for Whisper\"\"\"\n",
    "    \n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "def compute_metrics(pred, processor):\n",
    "    \"\"\"Compute WER metric during training\"\"\"\n",
    "    \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    # Replace -100 with processor's pad token id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    # Decode predictions and references\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Compute WER\n",
    "    wer = compute_wer(label_str, pred_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ZONE 4: EVALUATE - BASELINE VS FINE-TUNED\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    segment_id: str\n",
    "    ground_truth: str\n",
    "    baseline_prediction: str\n",
    "    finetuned_prediction: str\n",
    "    baseline_wer: float\n",
    "    finetuned_wer: float\n",
    "    improvement: float\n",
    "\n",
    "\n",
    "class WhisperEvaluator:\n",
    "    \"\"\"Evaluate baseline and fine-tuned models\"\"\"\n",
    "    \n",
    "    def __init__(self, processor, baseline_model, finetuned_model):\n",
    "        self.processor = processor\n",
    "        self.baseline_model = baseline_model\n",
    "        self.finetuned_model = finetuned_model\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.baseline_model.to(self.device)\n",
    "        self.finetuned_model.to(self.device)\n",
    "        self.baseline_model.eval()\n",
    "        self.finetuned_model.eval()\n",
    "    \n",
    "    def transcribe(self, audio_path: Path, model) -> str:\n",
    "        \"\"\"Transcribe with a model\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(str(audio_path), sr=16000)\n",
    "            \n",
    "            # Prepare input\n",
    "            inputs = self.processor(\n",
    "                audio,\n",
    "                sampling_rate=16000,\n",
    "                language=\"French\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_features = inputs.input_features.to(self.device)\n",
    "            \n",
    "            # Generate\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(input_features, language=\"<|fr|>\", task=\"transcribe\")\n",
    "            \n",
    "            # Decode\n",
    "            transcription = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            return transcription\n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸  Error: {audio_path.name} - {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def evaluate_dataset(self, dataset: Dataset, batch_size: int = 50) -> List[EvaluationResult]:\n",
    "        \"\"\"Evaluate baseline vs fine-tuned on dataset\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\nðŸ”„ Evaluating {len(dataset)} samples\\n\")\n",
    "        \n",
    "        for i, sample in enumerate(dataset):\n",
    "            audio_path = Path(sample[\"audio\"])\n",
    "            ground_truth = sample[\"text\"]\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            baseline_pred = self.transcribe(audio_path, self.baseline_model)\n",
    "            finetuned_pred = self.transcribe(audio_path, self.finetuned_model)\n",
    "            \n",
    "            # Calculate WER\n",
    "            baseline_wer = compute_wer(ground_truth, baseline_pred)\n",
    "            finetuned_wer = compute_wer(ground_truth, finetuned_pred)\n",
    "            improvement = baseline_wer - finetuned_wer\n",
    "            \n",
    "            results.append(EvaluationResult(\n",
    "                segment_id=f\"sample_{i:05d}\",\n",
    "                ground_truth=ground_truth,\n",
    "                baseline_prediction=baseline_pred,\n",
    "                finetuned_prediction=finetuned_pred,\n",
    "                baseline_wer=baseline_wer,\n",
    "                finetuned_wer=finetuned_wer,\n",
    "                improvement=improvement\n",
    "            ))\n",
    "            \n",
    "            if (i + 1) % batch_size == 0:\n",
    "                avg_baseline = sum(r.baseline_wer for r in results) / len(results)\n",
    "                avg_finetuned = sum(r.finetuned_wer for r in results) / len(results)\n",
    "                print(f\"  âœ“ {i + 1}/{len(dataset)} | Baseline WER: {avg_baseline:.3f} | Fine-tuned WER: {avg_finetuned:.3f}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def print_evaluation_report(results: List[EvaluationResult]):\n",
    "    \"\"\"Print detailed evaluation report\"\"\"\n",
    "    \n",
    "    baseline_wers = [r.baseline_wer for r in results]\n",
    "    finetuned_wers = [r.finetuned_wer for r in results]\n",
    "    improvements = [r.improvement for r in results]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š STEP 4: EVALUATION REPORT - BASELINE vs FINE-TUNED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Statistics:\")\n",
    "    print(f\"   Total segments: {len(results)}\")\n",
    "    \n",
    "    print(f\"\\n   BASELINE Model:\")\n",
    "    print(f\"      Avg WER:     {sum(baseline_wers) / len(baseline_wers):.3f}\")\n",
    "    print(f\"      Min WER:     {min(baseline_wers):.3f}\")\n",
    "    print(f\"      Max WER:     {max(baseline_wers):.3f}\")\n",
    "    print(f\"      Median WER:  {sorted(baseline_wers)[len(baseline_wers)//2]:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   FINE-TUNED Model:\")\n",
    "    print(f\"      Avg WER:     {sum(finetuned_wers) / len(finetuned_wers):.3f}\")\n",
    "    print(f\"      Min WER:     {min(finetuned_wers):.3f}\")\n",
    "    print(f\"      Max WER:     {max(finetuned_wers):.3f}\")\n",
    "    print(f\"      Median WER:  {sorted(finetuned_wers)[len(finetuned_wers)//2]:.3f}\")\n",
    "    \n",
    "    print(f\"\\n   IMPROVEMENT:\")\n",
    "    avg_improvement = sum(improvements) / len(improvements)\n",
    "    print(f\"      Avg improvement: {avg_improvement:.3f} WER points\")\n",
    "    print(f\"      Relative improvement: {(avg_improvement / (sum(baseline_wers) / len(baseline_wers))) * 100:.1f}%\")\n",
    "    \n",
    "    # Distribution\n",
    "    print(f\"\\n   Better performance:\")\n",
    "    better = sum(1 for imp in improvements if imp > 0)\n",
    "    worse = sum(1 for imp in improvements if imp < 0)\n",
    "    same = len(improvements) - better - worse\n",
    "    print(f\"      Improved:  {better} ({better/len(improvements)*100:.1f}%)\")\n",
    "    print(f\"      Worse:     {worse} ({worse/len(improvements)*100:.1f}%)\")\n",
    "    print(f\"      Same:      {same} ({same/len(improvements)*100:.1f}%)\")\n",
    "    \n",
    "    # Examples\n",
    "    print(f\"\\nðŸ“ Best improvements:\")\n",
    "    best = sorted(results, key=lambda x: x.improvement, reverse=True)[:3]\n",
    "    for i, r in enumerate(best, 1):\n",
    "        print(f\"      {i}. {r.ground_truth}\")\n",
    "        print(f\"         Baseline: {r.baseline_prediction} (WER: {r.baseline_wer:.3f})\")\n",
    "        print(f\"         Fine-tuned: {r.finetuned_prediction} (WER: {r.finetuned_wer:.3f})\")\n",
    "        print(f\"         Improvement: {r.improvement:.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "\n",
    "def save_evaluation_results(results: List[EvaluationResult], output_dir: Path):\n",
    "    \"\"\"Save evaluation results to JSON\"\"\"\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    data = [\n",
    "        {\n",
    "            \"segment_id\": r.segment_id,\n",
    "            \"ground_truth\": r.ground_truth,\n",
    "            \"baseline_prediction\": r.baseline_prediction,\n",
    "            \"finetuned_prediction\": r.finetuned_prediction,\n",
    "            \"baseline_wer\": r.baseline_wer,\n",
    "            \"finetuned_wer\": r.finetuned_wer,\n",
    "            \"improvement\": r.improvement\n",
    "        }\n",
    "        for r in results\n",
    "    ]\n",
    "    \n",
    "    with open(output_dir / \"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Evaluation results saved: {output_dir / 'evaluation_results.json'}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN: RUN COMPLETE FINE-TUNING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_finetuning_pipeline():\n",
    "    \"\"\"Run complete fine-tuning pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸš€ WHISPER FINE-TUNING PIPELINE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # ZONE 1: Load datasets\n",
    "    datasets = load_datasets_from_jsonl(CONFIG[\"dataset_dir\"])\n",
    "    \n",
    "    # Load processor\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“¦ LOADING PROCESSOR\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n  Loading processor from {CONFIG['model_name']}...\", end=\" \")\n",
    "    processor = WhisperProcessor.from_pretrained(CONFIG[\"model_name\"])\n",
    "    print(\"âœ“\\n\")\n",
    "    \n",
    "    # ZONE 2: Prepare datasets\n",
    "    prepared_datasets = prepare_datasets(datasets, processor, CONFIG)\n",
    "    \n",
    "    # ZONE 3: Fine-tune\n",
    "    finetuned_model = finetune_whisper(prepared_datasets, processor, CONFIG)\n",
    "    \n",
    "    # Save fine-tuned model\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ’¾ SAVE FINE-TUNED MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n  Saving model...\", end=\" \")\n",
    "    finetuned_model.save_pretrained(CONFIG[\"output_dir\"] / \"model\")\n",
    "    processor.save_pretrained(CONFIG[\"output_dir\"] / \"processor\")\n",
    "    print(\"âœ“\\n\")\n",
    "    \n",
    "    # ZONE 4: Evaluate\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ” COMPARING BASELINE vs FINE-TUNED\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load baseline\n",
    "    print(\"\\n  Loading baseline model...\", end=\" \")\n",
    "    baseline_model = WhisperForConditionalGeneration.from_pretrained(CONFIG[\"model_name\"])\n",
    "    print(\"âœ“\")\n",
    "    \n",
    "    evaluator = WhisperEvaluator(processor, baseline_model, finetuned_model)\n",
    "    \n",
    "    # Evaluate on eval dataset\n",
    "    results = evaluator.evaluate_dataset(datasets[\"eval\"], batch_size=50)\n",
    "    \n",
    "    # Print report\n",
    "    print_evaluation_report(results)\n",
    "    \n",
    "    # Save results\n",
    "    save_evaluation_results(results, CONFIG[\"output_dir\"])\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"âœ… FINE-TUNING PIPELINE COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ“‚ Output directory: {CONFIG['output_dir']}\")\n",
    "    print(\"\\nðŸ“‹ Generated files:\")\n",
    "    print(f\"   model/ ........................ Fine-tuned model\")\n",
    "    print(f\"   processor/ ................... Processor config\")\n",
    "    print(f\"   evaluation_results.json ...... Detailed results\")\n",
    "    print(\"\\nðŸŽ“ Fine-tuned model ready for deployment!\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_finetuning_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
