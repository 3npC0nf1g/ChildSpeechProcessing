{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "collapsed": false,
        "id": "96ebce18d578ee29"
      },
      "id": "96ebce18d578ee29"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y ffmpeg\n",
        "!pip install -U openai-whisper jiwer\n",
        "!pip install -U tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "97365edacf6fd6ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910b9600-0c1f-4eba-95a6-db487219e0b5"
      },
      "id": "97365edacf6fd6ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify Installation"
      ],
      "metadata": {
        "collapsed": false,
        "id": "95cc4187475103f6"
      },
      "id": "95cc4187475103f6"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "Whisper loaded: True\n",
            "FFmpeg OK\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Whisper loaded:\", whisper.load_model(\"base\") is not None)\n",
        "\n",
        "subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True)\n",
        "print(\"FFmpeg OK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a6ba7374799d70b",
        "outputId": "fa031493-12ef-4804-977b-a7dcd9df04c3"
      },
      "id": "7a6ba7374799d70b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "collapsed": false,
        "id": "12b9bbbb8e3a2ffa"
      },
      "id": "12b9bbbb8e3a2ffa"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c339a0388fd9f224",
        "outputId": "ad5ff831-b593-4360-c0e1-fa71f65408d8"
      },
      "id": "c339a0388fd9f224"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import and Configuration"
      ],
      "metadata": {
        "collapsed": false,
        "id": "2b1e25ed339c9857"
      },
      "id": "2b1e25ed339c9857"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass, asdict\n",
        "import whisper\n",
        "from jiwer import wer as compute_wer\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    \"cha_dir\": Path(\"/content/drive/MyDrive/asr/data/cha/1\"),\n",
        "    \"audio_dir\": Path(\"/content/drive/MyDrive/asr/data/songs/1\"),\n",
        "    \"output_dir\": Path(\"/content/drive/MyDrive/asr/output/whisper_children_dataset\"),\n",
        "    \"whisper_model\": \"base\",\n",
        "    \"sample_size\": None,  #\n",
        "    \"train_ratio\": 0.8,\n",
        "    \"sample_rate\": 16000,\n",
        "    \"audio_extensions\": [\".wav\", \".mp3\", \".m4a\", \".flac\"]\n",
        "}\n",
        "\n",
        "# CHILDES roles\n",
        "CHILD_ROLES = {\"Target_Child\", \"Child\", \"Sibling\", \"Peer\", \"Playmate\"}\n",
        "ADULT_ROLES = {\"Investigator\", \"Teacher\", \"Mother\", \"Father\", \"Adult\", \"Caregiver\", \"Parent\"}\n",
        "\n"
      ],
      "metadata": {
        "id": "f70d298f1e0c664f"
      },
      "id": "f70d298f1e0c664f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zone 1: File Matching"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7559a2d627494c09"
      },
      "id": "7559a2d627494c09"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "COMPLETE PIPELINE NOTEBOOK\n",
        "Input: Dataset folders (data/ca + data/songs)\n",
        "Output: Training dataset for Whisper fine-tuning (children voices only)\n",
        "\n",
        "Flow:\n",
        "1. Match .cha ↔ Audio files\n",
        "2. Extract .cha segments (word-level timestamps)\n",
        "3. Segment audio files based on timestamps\n",
        "4. Evaluate Whisper baseline (children only)\n",
        "5. Calculate WER (children only)\n",
        "6. Create training dataset (JSONL + metadata)\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "67462b483668a4bb"
      },
      "id": "67462b483668a4bb"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Union\n",
        "\n",
        "@dataclass\n",
        "class WorSegment:\n",
        "    speaker: str\n",
        "    text: str\n",
        "    words: list  # [(word, start, end)]\n",
        "    file_name: str = \"\"  # Ajouter le nom du fichier source\n",
        "\n",
        "\n",
        "def extract_wor_segments(path: Union[Path, str], debug: bool = False) -> List[WorSegment]:\n",
        "    \"\"\"\n",
        "    Extraire segments %wor d'un fichier .cha\n",
        "\n",
        "    Args:\n",
        "        path: Chemin vers un fichier .cha OU un dossier contenant des .cha\n",
        "        debug: Afficher les infos de parsing\n",
        "\n",
        "    Returns:\n",
        "        Liste de WorSegment\n",
        "    \"\"\"\n",
        "    path = Path(path)\n",
        "\n",
        "    if path.is_dir():\n",
        "        # Si c'est un dossier, traiter tous les .cha\n",
        "        return _extract_from_directory(path, debug=debug)\n",
        "    elif path.is_file():\n",
        "        # Si c'est un fichier, le traiter\n",
        "        return _extract_from_file(path, debug=debug)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Chemin invalide: {path}\")\n",
        "\n",
        "\n",
        "def _extract_from_directory(cha_dir: Path, debug: bool = False) -> List[WorSegment]:\n",
        "    \"\"\"Extraire de tous les fichiers .cha d'un dossier (récursivement)\"\"\"\n",
        "\n",
        "    # Chercher les .cha dans le dossier ET les sous-dossiers\n",
        "    cha_files = sorted(cha_dir.glob(\"*.cha\")) + sorted(cha_dir.glob(\"**/*.cha\"))\n",
        "    # Enlever les doublons\n",
        "    cha_files = sorted(set(cha_files))\n",
        "\n",
        "    if not cha_files:\n",
        "        print(f\"Aucun fichier .cha trouvé dans {cha_dir}\")\n",
        "        return []\n",
        "\n",
        "    if debug:\n",
        "        print(f\"Traitement de {len(cha_files)} fichiers .cha\\n\")\n",
        "\n",
        "    all_segments = []\n",
        "\n",
        "    for cha_file in cha_files:\n",
        "        if debug:\n",
        "            print(f\" {cha_file.name}...\", end=\" \")\n",
        "\n",
        "        segments = _extract_from_file(cha_file, debug=False)\n",
        "        all_segments.extend(segments)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"({len(segments)} segments)\")\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"Total: {len(all_segments)} segments de {len(cha_files)} fichiers\")\n",
        "        print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "    return all_segments\n"
      ],
      "metadata": {
        "id": "519ecf07a4bc9b62"
      },
      "id": "519ecf07a4bc9b62"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [
        "\n",
        "def _extract_from_file(cha_file: Path, debug: bool = False) -> List[WorSegment]:\n",
        "    \"\"\"Extraire de un seul fichier .cha\"\"\"\n",
        "\n",
        "    segments = []\n",
        "    current_speaker = None\n",
        "    file_name = cha_file.stem\n",
        "\n",
        "    with cha_file.open(encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "\n",
        "            # ── tour principal\n",
        "            if line.startswith(\"*\"):\n",
        "                current_speaker = line.split(\":\", 1)[0].replace(\"*\", \"\").strip()\n",
        "\n",
        "            # ── word tier\n",
        "            elif line.startswith(\"%wor:\"):\n",
        "                if not current_speaker:\n",
        "                    continue\n",
        "\n",
        "                # Extraire la partie après \"%wor:\"\n",
        "                wor_content = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "                # Nettoyer les caractères de contrôle\n",
        "                wor_content = wor_content.replace('\\x15', '')\n",
        "\n",
        "                # Parser simple : splitter par espaces et apparier word + timestamp\n",
        "                tokens = wor_content.split()\n",
        "\n",
        "                words = []\n",
        "                i = 0\n",
        "                while i < len(tokens):\n",
        "                    token = tokens[i]\n",
        "\n",
        "                    # Vérifier si c'est un timestamp (format XXXXX_XXXXX)\n",
        "                    if re.match(r\"^\\d{5,}_\\d{5,}$\", token):\n",
        "                        # C'est un timestamp → l'attacher au mot précédent\n",
        "                        if words:\n",
        "                            word, _, _ = words[-1]\n",
        "                            match = re.match(r\"(\\d+)_(\\d+)\", token)\n",
        "                            if match:\n",
        "                                start, end = int(match.group(1)), int(match.group(2))\n",
        "                                words[-1] = (word, start, end)\n",
        "                        i += 1\n",
        "                        continue\n",
        "\n",
        "                    # Sinon, c'est un mot\n",
        "                    words.append((token, None, None))\n",
        "                    i += 1\n",
        "\n",
        "                # Filtrer : garder seulement les mots avec timestamps\n",
        "                words_with_ts = [(w, s, e) for w, s, e in words if s is not None and e is not None]\n",
        "\n",
        "                if not words_with_ts:\n",
        "                    continue\n",
        "\n",
        "                # Nettoyer le texte : enlever les ponctuations isolées\n",
        "                clean_words = [w for w, _, _ in words_with_ts if w not in ('?', '.', ',', '!', '+...')]\n",
        "\n",
        "                if clean_words:\n",
        "                    clean_text = \" \".join(clean_words)\n",
        "\n",
        "                    segments.append(\n",
        "                        WorSegment(\n",
        "                            speaker=current_speaker,\n",
        "                            text=clean_text,\n",
        "                            words=words_with_ts,\n",
        "                            file_name=file_name\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "                    if debug and len(segments) <= 3:\n",
        "                        print(f\"\\n {current_speaker}\")\n",
        "                        print(f\"   Text: {clean_text[:70]}\")\n",
        "                        print(f\"   Words: {words_with_ts[:3]}...\")\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\n{'=' * 60}\")\n",
        "        print(f\"Total segments ({file_name}): {len(segments)}\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "2c0067a8814b550f"
      },
      "id": "2c0067a8814b550f"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "EXEMPLE 1: UN FICHIER\n",
            "============================================================\n",
            "\n",
            " KAT\n",
            "   Text: un escargot Dylan\n",
            "   Words: [('un', 10438, 10478), ('escargot', 10739, 10919), ('Dylan', 10919, 11419)]...\n",
            "\n",
            " KAT\n",
            "   Text: comment\n",
            "   Words: [('comment', 35270, 35770)]...\n",
            "\n",
            " WIL\n",
            "   Text: moi fais la fourmi moi\n",
            "   Words: [('moi', 38747, 39670), ('fais', 40011, 40612), ('la', 41896, 42136)]...\n",
            "\n",
            "============================================================\n",
            "Total segments (01-1a): 26\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXEMPLE 2: DOSSIER ENTIER\n",
            "============================================================\n",
            "Traitement de 112 fichiers .cha\n",
            "\n",
            " 01-1a.cha... (26 segments)\n",
            " 01-1b.cha... (8 segments)\n",
            " 01-2.cha... (211 segments)\n",
            " 01-3a.cha... (91 segments)\n",
            " 01-3b.cha... (71 segments)\n",
            " 01-3c.cha... (89 segments)\n",
            " 01-3d.cha... (181 segments)\n",
            " 01-4.cha... (82 segments)\n",
            " 02-5.cha... (217 segments)\n",
            " 02-7.cha... (397 segments)\n",
            " 02-8.cha... (8 segments)\n",
            " 02-9a.cha... (224 segments)\n",
            " 02-9b.cha... (118 segments)\n",
            " 02-9c.cha... (40 segments)\n",
            " 02-9d.cha... (138 segments)\n",
            " 02-9e.cha... (17 segments)\n",
            " 02-9f.cha... (67 segments)\n",
            " 03-10a.cha... (75 segments)\n",
            " 03-10b.cha... (130 segments)\n",
            " 03-13a.cha... (66 segments)\n",
            " 03-13b.cha... (98 segments)\n",
            " 03-13c.cha... (85 segments)\n",
            " 03-13d.cha... (29 segments)\n",
            " 03-13e.cha... (65 segments)\n",
            " 03-13f.cha... (65 segments)\n",
            " 03-13g.cha... (28 segments)\n",
            " 03-13h.cha... (58 segments)\n",
            " 03-14.cha... (14 segments)\n",
            " 04-16.cha... (127 segments)\n",
            " 04-18a.cha... (136 segments)\n",
            " 04-18b.cha... (63 segments)\n",
            " 04-19a.cha... (82 segments)\n",
            " 04-19b.cha... (150 segments)\n",
            " 04-19c.cha... (63 segments)\n",
            " 04-19d.cha... (154 segments)\n",
            " 04-19e.cha... (202 segments)\n",
            " 04-19f.cha... (156 segments)\n",
            " 05-20.cha... (275 segments)\n",
            " 05-22.cha... (172 segments)\n",
            " 05-23a.cha... (193 segments)\n",
            " 05-23b.cha... (118 segments)\n",
            " 05-23c.cha... (87 segments)\n",
            " 05-23d.cha... (142 segments)\n",
            " 05-23e.cha... (125 segments)\n",
            " 05-23f.cha... (66 segments)\n",
            " 05-23g.cha... (60 segments)\n",
            " 06-24a.cha... (167 segments)\n",
            " 06-24b.cha... (169 segments)\n",
            " 06-26a.cha... (268 segments)\n",
            " 06-26b.cha... (93 segments)\n",
            " 06-26c.cha... (96 segments)\n",
            " 06-26d.cha... (78 segments)\n",
            " 06-26e.cha... (90 segments)\n",
            " 07-27a.cha... (354 segments)\n",
            " 07-27b.cha... (205 segments)\n",
            " 07-29a.cha... (100 segments)\n",
            " 07-29b.cha... (229 segments)\n",
            " 07-29c.cha... (149 segments)\n",
            " 07-30a.cha... (169 segments)\n",
            " 07-30b.cha... (146 segments)\n",
            " 08-31a.cha... (177 segments)\n",
            " 08-31b.cha... (147 segments)\n",
            " 08-32a.cha... (117 segments)\n",
            " 08-32b.cha... (86 segments)\n",
            " 08-33a.cha... (179 segments)\n",
            " 08-33b.cha... (48 segments)\n",
            " 08-33c.cha... (164 segments)\n",
            " 09-34a.cha... (168 segments)\n",
            " 09-34b.cha... (191 segments)\n",
            " 09-36.cha... (237 segments)\n",
            " 09-37a.cha... (63 segments)\n",
            " 09-37b.cha... (85 segments)\n",
            " 09-37c.cha... (116 segments)\n",
            " 09-37d.cha... (80 segments)\n",
            " 10-38a.cha... (184 segments)\n",
            " 10-38b.cha... (314 segments)\n",
            " 10-38c.cha... (189 segments)\n",
            " 10-39.cha... (166 segments)\n",
            " 10-40a.cha... (139 segments)\n",
            " 10-40b.cha... (107 segments)\n",
            " 10-40c.cha... (98 segments)\n",
            " 10-40d.cha... (174 segments)\n",
            " 10-40e.cha... (40 segments)\n",
            " 10-40f.cha... (134 segments)\n",
            " 11-41a.cha... (147 segments)\n",
            " 11-41b.cha... (75 segments)\n",
            " 11-41c.cha... (98 segments)\n",
            " 11-41d.cha... (136 segments)\n",
            " 11-42.cha... (151 segments)\n",
            " 11-43a.cha... (49 segments)\n",
            " 11-43b.cha... (185 segments)\n",
            " 11-43c.cha... (199 segments)\n",
            " 12-44a.cha... (152 segments)\n",
            " 12-44b.cha... (132 segments)\n",
            " 12-44c.cha... (168 segments)\n",
            " 12-44d.cha... (108 segments)\n",
            " 12-44e.cha... (147 segments)\n",
            " 12-45.cha... (111 segments)\n",
            " 13-46a.cha... (119 segments)\n",
            " 13-46b.cha... (78 segments)\n",
            " 13-46c.cha... (98 segments)\n",
            " 13-46d.cha... (104 segments)\n",
            " 13-46e.cha... (159 segments)\n",
            " 13-47a.cha... (97 segments)\n",
            " 13-47b.cha... (81 segments)\n",
            " 13-48a.cha... (98 segments)\n",
            " 13-48b.cha... (78 segments)\n",
            " 13-48c.cha... (130 segments)\n",
            " 13-48d.cha... (63 segments)\n",
            " 13-49a.cha... (55 segments)\n",
            " 13-49b.cha... (97 segments)\n",
            " 13-49c.cha... (92 segments)\n",
            "\n",
            "============================================================\n",
            "Total: 13992 segments de 112 fichiers\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "STATISTIQUES\n",
            "============================================================\n",
            "\n",
            "Par speaker (28 speakers):\n",
            "   CAR                   16 segments |   49.6s audio\n",
            "   CHI                    2 segments |   11.0s audio\n",
            "   DYL                  333 segments |  682.2s audio\n",
            "   ELE                   41 segments |   90.9s audio\n",
            "   ELI                  183 segments |  370.3s audio\n",
            "   ENZ                  269 segments |  490.3s audio\n",
            "   KAT                  7250 segments | 16632.0s audio\n",
            "   KEL                  136 segments |  279.8s audio\n",
            "   KLE                   29 segments |   44.8s audio\n",
            "   KLO                  167 segments |  990.8s audio\n",
            "   LAN                  213 segments |  444.3s audio\n",
            "   LIN                  113 segments |  375.6s audio\n",
            "   LIZ                   79 segments |  145.4s audio\n",
            "   LOU                   36 segments |   80.6s audio\n",
            "   LSN                  181 segments |  450.7s audio\n",
            "   LUS                  1278 segments | 2828.8s audio\n",
            "   MAI                  735 segments | 2001.9s audio\n",
            "   MAS                  180 segments |  378.4s audio\n",
            "   MAT                  499 segments | 2428.1s audio\n",
            "   NIN                  640 segments | 1718.7s audio\n",
            "   PAR0                  65 segments |  171.2s audio\n",
            "   RIT                  123 segments |  207.3s audio\n",
            "   SAR                  249 segments |  714.8s audio\n",
            "   SOF                   17 segments |   51.7s audio\n",
            "   TAA                   13 segments |   27.0s audio\n",
            "   UNI                   11 segments |   10.3s audio\n",
            "   VIC                  412 segments | 1632.0s audio\n",
            "   WIL                  722 segments | 1703.1s audio\n",
            "\n",
            "Par fichier (112 fichiers):\n",
            "   01-1a                           26 segments |   52.6s audio\n",
            "   01-1b                            8 segments |   30.1s audio\n",
            "   01-2                           211 segments |  504.9s audio\n",
            "   01-3a                           91 segments |  182.9s audio\n",
            "   01-3b                           71 segments |  424.2s audio\n",
            "   01-3c                           89 segments |  421.4s audio\n",
            "   01-3d                          181 segments |  529.7s audio\n",
            "   01-4                            82 segments |  139.1s audio\n",
            "   02-5                           217 segments |  455.5s audio\n",
            "   02-7                           397 segments |  784.7s audio\n",
            "   02-8                             8 segments |   17.0s audio\n",
            "   02-9a                          224 segments |  495.7s audio\n",
            "   02-9b                          118 segments |  240.1s audio\n",
            "   02-9c                           40 segments |  118.9s audio\n",
            "   02-9d                          138 segments |  294.0s audio\n",
            "   02-9e                           17 segments |   36.7s audio\n",
            "   02-9f                           67 segments |  178.1s audio\n",
            "   03-10a                          75 segments |  147.5s audio\n",
            "   03-10b                         130 segments |  286.0s audio\n",
            "   03-13a                          66 segments |  174.6s audio\n",
            "   03-13b                          98 segments |  210.9s audio\n",
            "   03-13c                          85 segments |  197.2s audio\n",
            "   03-13d                          29 segments |   90.3s audio\n",
            "   03-13e                          65 segments |  171.2s audio\n",
            "   03-13f                          65 segments |   89.7s audio\n",
            "   03-13g                          28 segments |   72.1s audio\n",
            "   03-13h                          58 segments |  125.7s audio\n",
            "   03-14                           14 segments |   44.1s audio\n",
            "   04-16                          127 segments |  255.7s audio\n",
            "   04-18a                         136 segments |  292.4s audio\n",
            "   04-18b                          63 segments |  150.0s audio\n",
            "   04-19a                          82 segments |  164.5s audio\n",
            "   04-19b                         150 segments |  307.7s audio\n",
            "   04-19c                          63 segments |  115.4s audio\n",
            "   04-19d                         154 segments | 1156.2s audio\n",
            "   04-19e                         202 segments |  365.7s audio\n",
            "   04-19f                         156 segments |  372.3s audio\n",
            "   05-20                          275 segments |  539.8s audio\n",
            "   05-22                          172 segments |  322.4s audio\n",
            "   05-23a                         193 segments |  419.9s audio\n",
            "   05-23b                         118 segments |  234.9s audio\n",
            "   05-23c                          87 segments |  166.5s audio\n",
            "   05-23d                         142 segments |  401.8s audio\n",
            "   05-23e                         125 segments |  711.1s audio\n",
            "   05-23f                          66 segments |  484.5s audio\n",
            "   05-23g                          60 segments |  249.6s audio\n",
            "   06-24a                         167 segments |  698.9s audio\n",
            "   06-24b                         169 segments |  410.2s audio\n",
            "   06-26a                         268 segments |  945.5s audio\n",
            "   06-26b                          93 segments |  241.4s audio\n",
            "   06-26c                          96 segments |  204.7s audio\n",
            "   06-26d                          78 segments |  176.0s audio\n",
            "   06-26e                          90 segments |  217.6s audio\n",
            "   07-27a                         354 segments |  666.2s audio\n",
            "   07-27b                         205 segments |  501.8s audio\n",
            "   07-29a                         100 segments |  214.1s audio\n",
            "   07-29b                         229 segments |  423.3s audio\n",
            "   07-29c                         149 segments | 1317.8s audio\n",
            "   07-30a                         169 segments |  363.8s audio\n",
            "   07-30b                         146 segments |  339.4s audio\n",
            "   08-31a                         177 segments |  379.7s audio\n",
            "   08-31b                         147 segments |  325.4s audio\n",
            "   08-32a                         117 segments |  337.5s audio\n",
            "   08-32b                          86 segments |  228.4s audio\n",
            "   08-33a                         179 segments |  400.2s audio\n",
            "   08-33b                          48 segments |   99.9s audio\n",
            "   08-33c                         164 segments |  415.3s audio\n",
            "   09-34a                         168 segments |  403.5s audio\n",
            "   09-34b                         191 segments |  411.1s audio\n",
            "   09-36                          237 segments |  521.0s audio\n",
            "   09-37a                          63 segments |  155.2s audio\n",
            "   09-37b                          85 segments |  176.8s audio\n",
            "   09-37c                         116 segments |  304.6s audio\n",
            "   09-37d                          80 segments |  205.7s audio\n",
            "   10-38a                         184 segments |  361.6s audio\n",
            "   10-38b                         314 segments |  671.0s audio\n",
            "   10-38c                         189 segments |  378.7s audio\n",
            "   10-39                          166 segments |  407.6s audio\n",
            "   10-40a                         139 segments |  264.8s audio\n",
            "   10-40b                         107 segments |  222.8s audio\n",
            "   10-40c                          98 segments |  203.3s audio\n",
            "   10-40d                         174 segments |  363.5s audio\n",
            "   10-40e                          40 segments |   91.5s audio\n",
            "   10-40f                         134 segments |  296.1s audio\n",
            "   11-41a                         147 segments |  346.4s audio\n",
            "   11-41b                          75 segments |  186.4s audio\n",
            "   11-41c                          98 segments |  292.4s audio\n",
            "   11-41d                         136 segments |  313.4s audio\n",
            "   11-42                          151 segments |  379.9s audio\n",
            "   11-43a                          49 segments |  124.5s audio\n",
            "   11-43b                         185 segments |  381.6s audio\n",
            "   11-43c                         199 segments |  443.1s audio\n",
            "   12-44a                         152 segments |  360.2s audio\n",
            "   12-44b                         132 segments |  280.9s audio\n",
            "   12-44c                         168 segments |  330.9s audio\n",
            "   12-44d                         108 segments |  233.4s audio\n",
            "   12-44e                         147 segments |  332.7s audio\n",
            "   12-45                          111 segments |  267.2s audio\n",
            "   13-46a                         119 segments |  278.4s audio\n",
            "   13-46b                          78 segments |  155.1s audio\n",
            "   13-46c                          98 segments |  220.9s audio\n",
            "   13-46d                         104 segments |  246.0s audio\n",
            "   13-46e                         159 segments |  378.5s audio\n",
            "   13-47a                          97 segments |  221.6s audio\n",
            "   13-47b                          81 segments |  214.6s audio\n",
            "   13-48a                          98 segments |  294.4s audio\n",
            "   13-48b                          78 segments |  178.5s audio\n",
            "   13-48c                         130 segments |  293.9s audio\n",
            "   13-48d                          63 segments |  136.9s audio\n",
            "   13-49a                          55 segments |  137.6s audio\n",
            "   13-49b                          97 segments |  199.4s audio\n",
            "   13-49c                          92 segments |  211.6s audio\n",
            "\n",
            "Globales:\n",
            "   Total segments: 13992\n",
            "   Total audio: 583.5 minutes\n",
            "   Mots par segment (moyennes): 5.2\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "EXEMPLES DE SEGMENTS:\n",
            "============================================================\n",
            "\n",
            "[1] KAT (01-1a)\n",
            "  Text: un escargot Dylan\n",
            "  Time: 10438 → 11419 ms\n",
            "  Words: [('un', 10438, 10478), ('escargot', 10739, 10919), ('Dylan', 10919, 11419)]...\n",
            "\n",
            "[2] KAT (01-1a)\n",
            "  Text: comment\n",
            "  Time: 35270 → 35770 ms\n",
            "  Words: [('comment', 35270, 35770)]...\n",
            "\n",
            "[3] WIL (01-1a)\n",
            "  Text: moi fais la fourmi moi\n",
            "  Time: 38747 → 43579 ms\n",
            "  Words: [('moi', 38747, 39670), ('fais', 40011, 40612), ('la', 41896, 42136)]...\n",
            "\n",
            "[4] KAT (01-1a)\n",
            "  Text: alors et toi qu'estc(e) que tu fais Lucien\n",
            "  Time: 191460 → 193466 ms\n",
            "  Words: [('alors', 191460, 191580), ('et', 191580, 191620), ('toi', 191640, 191801)]...\n",
            "\n",
            "[5] KAT (01-1a)\n",
            "  Text: moi je m'appelle Katerina\n",
            "  Time: 193668 → 196237 ms\n",
            "  Words: [('moi', 193668, 193869), ('je', 193990, 194070), (\"m'appelle\", 194371, 195636)]...\n"
          ]
        }
      ],
      "source": [
        "def print_statistics(segments: List[WorSegment]):\n",
        "    \"\"\"Afficher statistiques détaillées sur les segments\"\"\"\n",
        "\n",
        "    if not segments:\n",
        "        print(\" Aucun segment trouvé\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(\"STATISTIQUES\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    # Par speaker\n",
        "    by_speaker = {}\n",
        "    by_file = {}\n",
        "\n",
        "    for seg in segments:\n",
        "        # Par speaker\n",
        "        by_speaker.setdefault(seg.speaker, []).append(seg)\n",
        "\n",
        "        # Par fichier\n",
        "        by_file.setdefault(seg.file_name, []).append(seg)\n",
        "\n",
        "    print(f\"\\nPar speaker ({len(by_speaker)} speakers):\")\n",
        "    for speaker in sorted(by_speaker.keys()):\n",
        "        segs = by_speaker[speaker]\n",
        "        total_duration = sum(s.words[-1][2] - s.words[0][1] for s in segs) / 1000\n",
        "        print(f\"   {speaker:20} {len(segs):3d} segments | {total_duration:6.1f}s audio\")\n",
        "\n",
        "    print(f\"\\nPar fichier ({len(by_file)} fichiers):\")\n",
        "    for file_name in sorted(by_file.keys()):\n",
        "        segs = by_file[file_name]\n",
        "        total_duration = sum(s.words[-1][2] - s.words[0][1] for s in segs) / 1000\n",
        "        print(f\"   {file_name:30} {len(segs):3d} segments | {total_duration:6.1f}s audio\")\n",
        "\n",
        "    # Stats globales\n",
        "    total_duration = sum(s.words[-1][2] - s.words[0][1] for s in segments) / 1000 / 60\n",
        "    avg_words = sum(len(s.words) for s in segments) / len(segments)\n",
        "\n",
        "    print(f\"\\nGlobales:\")\n",
        "    print(f\"   Total segments: {len(segments)}\")\n",
        "    print(f\"   Total audio: {total_duration:.1f} minutes\")\n",
        "    print(f\"   Mots par segment (moyennes): {avg_words:.1f}\")\n",
        "    print(f\"{'=' * 60}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Exemple 1: Traiter UN fichier\n",
        "    print(\"=\" * 60)\n",
        "    print(\"EXEMPLE 1: UN FICHIER\")\n",
        "    print(\"=\" * 60)\n",
        "    segments_single = extract_wor_segments(Path(\"/content/drive/MyDrive/asr/data/cha/1/01-1a.cha\"), debug=True)\n",
        "\n",
        "    # Exemple 2: Traiter UN DOSSIER ENTIER\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"EXEMPLE 2: DOSSIER ENTIER\")\n",
        "    print(\"=\" * 60)\n",
        "    segments_all = extract_wor_segments(Path(\"/content/drive/MyDrive/asr/data/cha/1/\"), debug=True)\n",
        "\n",
        "    # Afficher statistiques\n",
        "    print_statistics(segments_all)\n",
        "\n",
        "    # Exemples\n",
        "    if segments_all:\n",
        "        print(f\"{'=' * 60}\")\n",
        "        print(\"EXEMPLES DE SEGMENTS:\")\n",
        "        print(f\"{'=' * 60}\")\n",
        "        for i, seg in enumerate(segments_all[:5]):\n",
        "            print(f\"\\n[{i + 1}] {seg.speaker} ({seg.file_name})\")\n",
        "            print(f\"  Text: {seg.text}\")\n",
        "            if seg.words:\n",
        "                print(f\"  Time: {seg.words[0][1]} → {seg.words[-1][2]} ms\")\n",
        "                print(f\"  Words: {seg.words[:3]}...\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47b2fd6c188fbf24",
        "outputId": "724aa4b5-6ebc-48ac-deed-a14e17ba3d86"
      },
      "id": "47b2fd6c188fbf24"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "source": [
        "def find_matching_files(cha_dir: Path, audio_dir: Path, extensions: List[str]) -> Dict:\n",
        "    \"\"\"Match .cha with audio files by relative path (respects subdirectories)\"\"\"\n",
        "\n",
        "    cha_files = sorted(cha_dir.glob(\"**/*.cha\"))\n",
        "    audio_files = []\n",
        "    for ext in extensions:\n",
        "        audio_files.extend(audio_dir.glob(f\"**/*{ext}\"))\n",
        "\n",
        "    # Créer dicts: relative_path_with_stem → fichier\n",
        "    # Exemple: \"1/01-1a\" pour data/cha/1/01-1a.cha\n",
        "    cha_by_path = {}\n",
        "    for f in cha_files:\n",
        "        relative_stem = str(f.relative_to(cha_dir).with_suffix(\"\"))  # \"1/01-1a\"\n",
        "        cha_by_path[relative_stem] = f\n",
        "\n",
        "    audio_by_path = {}\n",
        "    for f in audio_files:\n",
        "        relative_stem = str(f.relative_to(audio_dir).with_suffix(\"\"))\n",
        "        audio_by_path[relative_stem] = f\n",
        "\n",
        "    # Matcher: chercher les mêmes chemins relatifs\n",
        "    matched = []\n",
        "    cha_missing = []\n",
        "    audio_orphans = []\n",
        "\n",
        "    for relative_path in cha_by_path:\n",
        "        if relative_path in audio_by_path:\n",
        "            matched.append((cha_by_path[relative_path], audio_by_path[relative_path]))\n",
        "        else:\n",
        "            cha_missing.append(cha_by_path[relative_path])\n",
        "\n",
        "    for relative_path in audio_by_path:\n",
        "        if relative_path not in cha_by_path:\n",
        "            audio_orphans.append(audio_by_path[relative_path])\n",
        "\n",
        "    return {\n",
        "        \"matched\": matched,\n",
        "        \"cha_missing_audio\": cha_missing,\n",
        "        \"audio_orphans\": audio_orphans,\n",
        "        \"total_cha\": len(cha_files),\n",
        "        \"total_audio\": len(audio_files),\n",
        "        \"matched_count\": len(matched)\n",
        "    }\n",
        "\n",
        "\n",
        "def print_matching_report(result: Dict):\n",
        "    \"\"\"Afficher le rapport de matching\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: FILE MATCHING (by relative path)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n Found:\")\n",
        "    print(f\"   Total .cha files:       {result['total_cha']}\")\n",
        "    print(f\"   Total audio files:      {result['total_audio']}\")\n",
        "    print(f\"   Matched pairs:        {result['matched_count']}\")\n",
        "    print(f\"   .cha missing audio:  {len(result['cha_missing_audio'])}\")\n",
        "    print(f\"   Audio orphans:       {len(result['audio_orphans'])}\")\n",
        "\n",
        "    if result['cha_missing_audio']:\n",
        "        print(f\"\\n   Missing audio for:\")\n",
        "        for cha in result['cha_missing_audio'][:10]:\n",
        "            print(f\"      - {cha.name}\")\n",
        "        if len(result['cha_missing_audio']) > 10:\n",
        "            print(f\"      ... and {len(result['cha_missing_audio']) - 10} more\")\n",
        "\n",
        "    if result['audio_orphans']:\n",
        "        print(f\"\\n   Audio without .cha:\")\n",
        "        for audio in result['audio_orphans'][:10]:\n",
        "            print(f\"      - {audio.name}\")\n",
        "        if len(result['audio_orphans']) > 10:\n",
        "            print(f\"      ... and {len(result['audio_orphans']) - 10} more\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "fd3a77bf04214636"
      },
      "id": "fd3a77bf04214636"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zone 2: Segment Extraction"
      ],
      "metadata": {
        "collapsed": false,
        "id": "c14693053fe52933"
      },
      "id": "c14693053fe52933"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "source": [
        "def extract_segments_from_matched(matched_pairs: List[Tuple[Path, Path]]) -> List[WorSegment]:\n",
        "    \"\"\"Extract .cha segments from matched files only\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"STEP 2: EXTRACT .CHA SEGMENTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    all_segments = []\n",
        "\n",
        "    for i, (cha_file, audio_file) in enumerate(matched_pairs):\n",
        "        segments = extract_wor_segments(cha_file, debug=False)\n",
        "        all_segments.extend(segments)\n",
        "\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f\"  ✓ {i + 1}/{len(matched_pairs)} files\")\n",
        "\n",
        "    print(f\"\\nExtracted {len(all_segments)} segments with timestamps\\n\")\n",
        "    return all_segments"
      ],
      "metadata": {
        "id": "2a740ee2268e7c80"
      },
      "id": "2a740ee2268e7c80"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zone 3: Audio Segmentation"
      ],
      "metadata": {
        "collapsed": false,
        "id": "e8c46df2bd7edb24"
      },
      "id": "e8c46df2bd7edb24"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "AUDIO SEGMENTER WITH MAX LIMIT\n",
        "Permet de limiter le nombre de segments à traiter pour respecter les limites de calcul\n",
        "\"\"\"\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class AudioSegmenter:\n",
        "    \"\"\"Segment audio files based on timestamps with optional limit\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: Path, sample_rate: int = 16000):\n",
        "        self.output_dir = output_dir\n",
        "        self.sample_rate = sample_rate\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.speaker_dirs = {}\n",
        "\n",
        "    def _get_speaker_dir(self, speaker: str) -> Path:\n",
        "        \"\"\"Get or create speaker directory\"\"\"\n",
        "        if speaker not in self.speaker_dirs:\n",
        "            d = self.output_dir / speaker\n",
        "            d.mkdir(exist_ok=True)\n",
        "            self.speaker_dirs[speaker] = d\n",
        "        return self.speaker_dirs[speaker]\n",
        "\n",
        "    def extract_segment(self, audio_file: Path, start_ms: int, end_ms: int, output_path: Path) -> bool:\n",
        "        \"\"\"Extract audio segment using ffmpeg\"\"\"\n",
        "        if not audio_file.exists():\n",
        "            return False\n",
        "\n",
        "        start_sec = start_ms / 1000.0\n",
        "        duration_sec = (end_ms - start_ms) / 1000.0\n",
        "\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-i\", str(audio_file),\n",
        "            \"-ss\", str(start_sec), \"-t\", str(duration_sec),\n",
        "            \"-acodec\", \"pcm_s16le\", \"-ar\", str(self.sample_rate), \"-ac\", \"1\",\n",
        "            \"-y\", str(output_path)\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            subprocess.run(cmd, check=True, capture_output=True, timeout=10)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def segment_all(self, segments: List[WorSegment], matched_pairs: List[Tuple[Path, Path]],\n",
        "                   max_segments: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Segment audio files with optional limit\n",
        "\n",
        "        Args:\n",
        "            segments: List of WorSegment objects\n",
        "            matched_pairs: List of (cha_file, audio_file) tuples\n",
        "            max_segments: Maximum number of segments to process (None = all)\n",
        "\n",
        "        Returns:\n",
        "            Dict with:\n",
        "            - extracted: List of successfully extracted segments\n",
        "            - skipped: Number of skipped segments (no audio file)\n",
        "            - stopped_at: Number processed before stopping (if max reached)\n",
        "            - limited: Boolean, whether max_segments was reached\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\" STEP 3: SEGMENT AUDIO FILES\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        audio_lookup = {audio.stem: audio for _, audio in matched_pairs}\n",
        "\n",
        "        # Déterminer le nombre de segments à traiter\n",
        "        segments_to_process = segments\n",
        "        limited = False\n",
        "\n",
        "        if max_segments is not None:\n",
        "            if len(segments) > max_segments:\n",
        "                segments_to_process = segments[:max_segments]\n",
        "                limited = True\n",
        "                print(f\"\\n LIMIT SET: Processing {max_segments}/{len(segments)} segments\")\n",
        "            else:\n",
        "                print(f\"\\n Processing all {len(segments)} segments (limit: {max_segments})\")\n",
        "        else:\n",
        "            print(f\"\\n Processing all {len(segments)} segments (no limit)\")\n",
        "\n",
        "        results = []\n",
        "        skipped = 0\n",
        "\n",
        "        for i, seg in enumerate(segments_to_process):\n",
        "            audio_file = audio_lookup.get(seg.file_name)\n",
        "            if not audio_file or not audio_file.exists():\n",
        "                skipped += 1\n",
        "                continue\n",
        "\n",
        "            segment_id = f\"{seg.file_name}_{seg.speaker}_{i:05d}\"\n",
        "            speaker_dir = self._get_speaker_dir(seg.speaker)\n",
        "            output_path = speaker_dir / f\"{segment_id}.wav\"\n",
        "\n",
        "            start_ms = seg.words[0][1]\n",
        "            end_ms = seg.words[-1][2]\n",
        "\n",
        "            if self.extract_segment(audio_file, start_ms, end_ms, output_path):\n",
        "                results.append({\n",
        "                    \"segment_id\": segment_id,\n",
        "                    \"speaker\": seg.speaker,\n",
        "                    \"file_name\": seg.file_name,\n",
        "                    \"audio_path\": str(output_path),\n",
        "                    \"duration_ms\": end_ms - start_ms,\n",
        "                    \"text\": seg.text,\n",
        "                    \"num_words\": len(seg.words)\n",
        "                })\n",
        "\n",
        "            if (i + 1) % 200 == 0:\n",
        "                print(f\"  ✓ {i + 1}/{len(segments_to_process)} segments\")\n",
        "\n",
        "        print(f\"\\n Segmented {len(results)} audio files\")\n",
        "        if skipped > 0:\n",
        "            print(f\"   Skipped (no audio): {skipped}\")\n",
        "\n",
        "        if limited:\n",
        "            print(f\"   STOPPED AT LIMIT: {len(segments_to_process)} processed\")\n",
        "            print(f\"   Remaining: {len(segments) - len(segments_to_process)} segments not processed\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        return {\n",
        "            \"extracted\": results,\n",
        "            \"skipped\": skipped,\n",
        "            \"stopped_at\": len(segments_to_process),\n",
        "            \"limited\": limited,\n",
        "            \"total_segments\": len(segments)\n",
        "        }\n",
        "\n",
        "# ============================================================================\n",
        "# HELPER: Resume segmentation\n",
        "# ============================================================================\n",
        "\n",
        "def resume_segmentation(segments: List[WorSegment], matched_pairs: List[Tuple[Path, Path]],\n",
        "                       output_dir: Path, start_from: int = 0, max_segments: int = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Resume segmentation from a specific point\n",
        "\n",
        "    Utile si tu veux reprendre après avoir atteint ta limite\n",
        "\n",
        "    Args:\n",
        "        segments: List of all WorSegment objects\n",
        "        matched_pairs: List of (cha_file, audio_file) tuples\n",
        "        output_dir: Output directory\n",
        "        start_from: Index to start from (0 = beginning)\n",
        "        max_segments: Maximum segments to process from start_from\n",
        "\n",
        "    Returns:\n",
        "        Dict with results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"RESUME SEGMENTATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n  Resuming from segment {start_from}\")\n",
        "    print(f\"  Total remaining: {len(segments) - start_from}\\n\")\n",
        "\n",
        "    # Get remaining segments\n",
        "    remaining_segments = segments[start_from:]\n",
        "\n",
        "    segmenter = AudioSegmenter(output_dir)\n",
        "    results = segmenter.segment_all(remaining_segments, matched_pairs, max_segments)\n",
        "\n",
        "    # Adjust stopped_at to reflect actual position in original list\n",
        "    results[\"stopped_at\"] = start_from + results[\"stopped_at\"]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "80497d00e612e1b2"
      },
      "id": "80497d00e612e1b2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zone 4: Extract Speakers Metadata"
      ],
      "metadata": {
        "collapsed": false,
        "id": "943d4e4e3bfda994"
      },
      "id": "943d4e4e3bfda994"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "source": [
        "def extract_all_speakers_info(matched_pairs: List[Tuple[Path, Path]]) -> Dict[str, str]:\n",
        "    \"\"\"Extract speaker roles from all .cha files\"\"\"\n",
        "\n",
        "    all_speakers = {}\n",
        "\n",
        "    for cha_file, _ in matched_pairs:\n",
        "        with cha_file.open(encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                if line.startswith(\"@Participants:\"):\n",
        "                    participants_str = line.split(\":\", 1)[1].strip()\n",
        "                    for participant in participants_str.split(\",\"):\n",
        "                        participant = participant.strip()\n",
        "                        parts = participant.rsplit(\" \", 1)\n",
        "                        if len(parts) == 2:\n",
        "                            speaker_name, role = parts\n",
        "                            all_speakers[speaker_name] = role\n",
        "\n",
        "    return all_speakers"
      ],
      "metadata": {
        "id": "8396f12238990506"
      },
      "id": "8396f12238990506"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zone 5: Whisper Baseline Evaluation Compute of the WER before fine-tuning"
      ],
      "metadata": {
        "collapsed": false,
        "id": "12fa0821d60383a"
      },
      "id": "12fa0821d60383a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whisper Evaluation Class"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7d74957261de05d4"
      },
      "id": "7d74957261de05d4"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TranscriptionResult:\n",
        "    segment_id: str\n",
        "    speaker: str\n",
        "    file_name: str\n",
        "    audio_path: str\n",
        "    ground_truth: str\n",
        "    whisper_prediction: str\n",
        "    duration_ms: int\n",
        "    wer: float\n",
        "    confidence: float = 0.0\n",
        "\n",
        "\n",
        "class WhisperEvaluator:\n",
        "    \"\"\"Evaluate Whisper baseline on children voices\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"base\"):\n",
        "        print(f\"Loading Whisper '{model_name}'...\")\n",
        "        self.model = whisper.load_model(model_name)\n",
        "        print(\"Loaded\\n\")\n",
        "\n",
        "    def transcribe(self, audio_path: Path) -> Dict:\n",
        "        if not audio_path.exists():\n",
        "            return {\"text\": \"\", \"confidence\": 0.0}\n",
        "        try:\n",
        "            result = self.model.transcribe(str(audio_path), language=\"fr\", verbose=False)\n",
        "            return {\"text\": result[\"text\"].strip(), \"confidence\": result.get(\"confidence\", 0.0)}\n",
        "        except:\n",
        "            return {\"text\": \"\", \"confidence\": 0.0}\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_wer(ground_truth: str, prediction: str) -> float:\n",
        "        if not ground_truth.strip():\n",
        "            return 0.0 if not prediction.strip() else 1.0\n",
        "        return compute_wer(ground_truth, prediction)\n",
        "\n",
        "    def evaluate_children(self, audio_segments: List[Dict], speakers_info: Dict,\n",
        "                         sample_size: int = None) -> List[TranscriptionResult]:\n",
        "        \"\"\"Evaluate only children speakers\"\"\"\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"STEP 4: WHISPER BASELINE EVALUATION (CHILDREN ONLY)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Filter children only\n",
        "        children_segments = [s for s in audio_segments\n",
        "                            if speakers_info.get(s[\"speaker\"]) in CHILD_ROLES]\n",
        "\n",
        "        if sample_size:\n",
        "            children_segments = children_segments[:sample_size]\n",
        "\n",
        "        print(f\"\\nEvaluating {len(children_segments)} children segments\\n\")\n",
        "\n",
        "        results = []\n",
        "        for i, seg in enumerate(children_segments):\n",
        "            audio_path = Path(seg[\"audio_path\"])\n",
        "            transcription = self.transcribe(audio_path)\n",
        "            wer = self.calculate_wer(seg[\"text\"], transcription[\"text\"])\n",
        "\n",
        "            results.append(TranscriptionResult(\n",
        "                segment_id=seg[\"segment_id\"],\n",
        "                speaker=seg[\"speaker\"],\n",
        "                file_name=seg[\"file_name\"],\n",
        "                audio_path=seg[\"audio_path\"],\n",
        "                ground_truth=seg[\"text\"],\n",
        "                whisper_prediction=transcription[\"text\"],\n",
        "                duration_ms=seg[\"duration_ms\"],\n",
        "                wer=wer,\n",
        "                confidence=transcription[\"confidence\"]\n",
        "            ))\n",
        "\n",
        "            if (i + 1) % 50 == 0:\n",
        "                avg_wer = sum(r.wer for r in results) / len(results)\n",
        "                print(f\"  ✓ {i + 1}/{len(children_segments)} | Avg WER: {avg_wer:.3f}\")\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "2f0f5c05dcf4a3ab"
      },
      "id": "2f0f5c05dcf4a3ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WER Report"
      ],
      "metadata": {
        "collapsed": false,
        "id": "74cade2ed08b42c4"
      },
      "id": "74cade2ed08b42c4"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "\n",
        "def print_wer_report(results: List[TranscriptionResult], speakers_info: Dict):\n",
        "    \"\"\"Print WER statistics\"\"\"\n",
        "\n",
        "    wers = [r.wer for r in results]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 5: WER STATISTICS (CHILDREN ONLY)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(f\"\\nGlobal:\")\n",
        "    print(f\"   Total segments:  {len(results)}\")\n",
        "    print(f\"   Avg WER:         {sum(wers) / len(wers):.3f}\")\n",
        "    print(f\"   Min WER:         {min(wers):.3f}\")\n",
        "    print(f\"   Max WER:         {max(wers):.3f}\")\n",
        "    print(f\"   Median WER:      {sorted(wers)[len(wers)//2]:.3f}\")\n",
        "\n",
        "    print(f\"\\n   Distribution:\")\n",
        "    for low, high in [(0.0, 0.1), (0.1, 0.3), (0.3, 0.5), (0.5, 1.0)]:\n",
        "        count = sum(1 for w in wers if low <= w < high)\n",
        "        pct = (count / len(wers)) * 100\n",
        "        print(f\"      {low:.1f}-{high:.1f}: {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "    # By speaker\n",
        "    by_speaker = {}\n",
        "    for r in results:\n",
        "        by_speaker.setdefault(r.speaker, []).append(r.wer)\n",
        "\n",
        "    print(f\"\\n By speaker ({len(by_speaker)}):\")\n",
        "    for speaker in sorted(by_speaker.keys()):\n",
        "        wers_sp = by_speaker[speaker]\n",
        "        avg = sum(wers_sp) / len(wers_sp)\n",
        "        print(f\"      {speaker:15} {len(wers_sp):4d} segments | WER: {avg:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "79b8d8b8b8619ebb"
      },
      "id": "79b8d8b8b8619ebb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zone 6: Create Training Dataset"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7e608ae279a9c06a"
      },
      "id": "7e608ae279a9c06a"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "source": [
        "class DatasetBuilder:\n",
        "    \"\"\"Create train/test splits for fine-tuning\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir: Path):\n",
        "        self.output_dir = output_dir\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def create_dataset(self, results: List[TranscriptionResult], train_ratio: float = 0.8):\n",
        "        \"\"\"Create JSONL + metadata files\"\"\"\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"STEP 6: CREATE TRAINING DATASET\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        split_idx = int(len(results) * train_ratio)\n",
        "        train = results[:split_idx]\n",
        "        test = results[split_idx:]\n",
        "\n",
        "        print(f\"\\nDataset split:\")\n",
        "        print(f\"   Total:   {len(results)} segments\")\n",
        "        print(f\"   Train:   {len(train)} segments ({train_ratio*100:.0f}%)\")\n",
        "        print(f\"   Test:    {len(test)} segments ({(1-train_ratio)*100:.0f}%)\")\n",
        "\n",
        "        # Save JSONL (for fine-tuning)\n",
        "        self._save_jsonl(train, self.output_dir / \"train.jsonl\")\n",
        "        self._save_jsonl(test, self.output_dir / \"eval.jsonl\")\n",
        "\n",
        "        # Save metadata JSON (for analysis)\n",
        "        self._save_metadata(train, self.output_dir / \"train_metadata.json\")\n",
        "        self._save_metadata(test, self.output_dir / \"eval_metadata.json\")\n",
        "\n",
        "        print(f\"\\nDataset created in {self.output_dir}\\n\")\n",
        "\n",
        "    def _save_jsonl(self, results: List[TranscriptionResult], output_file: Path):\n",
        "        \"\"\"Save as JSONL for Whisper\"\"\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for r in results:\n",
        "                entry = {\"audio\": r.audio_path, \"text\": r.ground_truth, \"language\": \"fr\"}\n",
        "                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "        print(f\"   ✓ {output_file.name} ({len(results)} segments)\")\n",
        "\n",
        "    def _save_metadata(self, results: List[TranscriptionResult], output_file: Path):\n",
        "        \"\"\"Save complete metadata\"\"\"\n",
        "        data = [asdict(r) for r in results]\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"   ✓ {output_file.name}\")"
      ],
      "metadata": {
        "id": "12ce2b08fa79f652"
      },
      "id": "12ce2b08fa79f652"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN PIPELINE"
      ],
      "metadata": {
        "collapsed": false,
        "id": "46f53541369e0d97"
      },
      "id": "46f53541369e0d97"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "COMPLETE WHISPER CHILDREN DATASET PIPELINE\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 1: FILE MATCHING (by relative path)\n",
            "======================================================================\n",
            "\n",
            " Found:\n",
            "   Total .cha files:       112\n",
            "   Total audio files:      112\n",
            "   Matched pairs:        112\n",
            "   .cha missing audio:  0\n",
            "   Audio orphans:       0\n",
            "\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 2: EXTRACT .CHA SEGMENTS\n",
            "======================================================================\n",
            "  ✓ 50/112 files\n",
            "  ✓ 100/112 files\n",
            "\n",
            "Extracted 13992 segments with timestamps\n",
            "\n",
            "======================================================================\n",
            "🎵 STEP 3: SEGMENT AUDIO FILES\n",
            "======================================================================\n",
            "\n",
            "⚠️  LIMIT SET: Processing 100/13992 segments\n",
            "\n",
            "✅ Segmented 100 audio files\n",
            "   ⚠️  STOPPED AT LIMIT: 100 processed\n",
            "   Remaining: 13892 segments not processed\n",
            "\n",
            "Loading Whisper 'base'...\n",
            "Loaded\n",
            "\n",
            "======================================================================\n",
            "STEP 4: WHISPER BASELINE EVALUATION (CHILDREN ONLY)\n",
            "======================================================================\n",
            "\n",
            "Evaluating 29 children segments\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 483/483 [00:01<00:00, 472.44frames/s]\n",
            "100%|██████████| 266/266 [00:00<00:00, 287.94frames/s]\n",
            "100%|██████████| 250/250 [00:00<00:00, 390.97frames/s]\n",
            "  0%|          | 0/78 [00:00<?, ?frames/s]\n",
            "  0%|          | 0/56 [00:00<?, ?frames/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 723.85frames/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 827.97frames/s]\n",
            "100%|██████████| 314/314 [00:04<00:00, 78.31frames/s]\n",
            "  0%|          | 0/50 [00:00<?, ?frames/s]\n",
            "  0%|          | 0/54 [00:00<?, ?frames/s]\n",
            "  0%|          | 0/50 [00:00<?, ?frames/s]\n",
            "100%|██████████| 805/805 [00:00<00:00, 2639.43frames/s]\n",
            "  0%|          | 0/295 [00:00<?, ?frames/s]\n",
            "  0%|          | 0/58 [00:00<?, ?frames/s]\n",
            "100%|██████████| 86/86 [00:00<00:00, 154.64frames/s]\n",
            "100%|██████████| 222/222 [00:00<00:00, 301.46frames/s]\n",
            "100%|██████████| 216/216 [00:00<00:00, 2019.20frames/s]\n",
            "100%|██████████| 144/144 [00:01<00:00, 104.23frames/s]\n",
            "100%|██████████| 134/134 [00:00<00:00, 168.88frames/s]\n",
            "100%|██████████| 334/334 [00:03<00:00, 101.35frames/s]\n",
            "100%|██████████| 464/464 [00:01<00:00, 429.03frames/s]\n",
            "  0%|          | 0/50 [00:00<?, ?frames/s]\n",
            "100%|██████████| 292/292 [00:00<00:00, 309.93frames/s]\n",
            "100%|██████████| 322/322 [00:00<00:00, 1417.37frames/s]\n",
            "100%|██████████| 254/254 [00:00<00:00, 335.64frames/s]\n",
            "100%|██████████| 1099/1099 [00:03<00:00, 319.46frames/s]\n",
            "100%|██████████| 208/208 [00:00<00:00, 394.40frames/s]\n",
            "100%|██████████| 144/144 [00:00<00:00, 174.71frames/s]\n",
            "  0%|          | 0/372 [00:00<?, ?frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 5: WER STATISTICS (CHILDREN ONLY)\n",
            "======================================================================\n",
            "\n",
            "Global:\n",
            "   Total segments:  29\n",
            "   Avg WER:         1.314\n",
            "   Min WER:         0.800\n",
            "   Max WER:         4.889\n",
            "   Median WER:      1.000\n",
            "\n",
            "   Distribution:\n",
            "      0.0-0.1:    0 (  0.0%)\n",
            "      0.1-0.3:    0 (  0.0%)\n",
            "      0.3-0.5:    0 (  0.0%)\n",
            "      0.5-1.0:    2 (  6.9%)\n",
            "\n",
            " By speaker (6):\n",
            "      CHI                2 segments | WER: 1.786\n",
            "      ELI                4 segments | WER: 2.056\n",
            "      MAT               17 segments | WER: 1.135\n",
            "      RIT                2 segments | WER: 1.500\n",
            "      UNI                3 segments | WER: 1.000\n",
            "      WIL                1 segments | WER: 1.000\n",
            "\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 6: CREATE TRAINING DATASET\n",
            "======================================================================\n",
            "\n",
            "Dataset split:\n",
            "   Total:   29 segments\n",
            "   Train:   23 segments (80%)\n",
            "   Test:    6 segments (20%)\n",
            "   ✓ train.jsonl (23 segments)\n",
            "   ✓ eval.jsonl (6 segments)\n",
            "   ✓ train_metadata.json\n",
            "   ✓ eval_metadata.json\n",
            "\n",
            "Dataset created in /content/drive/MyDrive/asr/output/whisper_children_dataset/training_dataset\n",
            "\n",
            "======================================================================\n",
            "PIPELINE COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Output directory: /content/drive/MyDrive/asr/output/whisper_children_dataset\n",
            "\n",
            "Files generated:\n",
            "   training_dataset/train.jsonl ........... for fine-tuning\n",
            "   training_dataset/eval.jsonl ........... for evaluation\n",
            "   training_dataset/train_metadata.json .. complete metadata\n",
            "   training_dataset/eval_metadata.json ... complete metadata\n",
            "\n",
            "Ready for Whisper fine-tuning!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def run_pipeline():\n",
        "    \"\"\"Run complete pipeline end-to-end\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMPLETE WHISPER CHILDREN DATASET PIPELINE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ZONE 1: Matching\n",
        "    match_result = find_matching_files(CONFIG[\"cha_dir\"], CONFIG[\"audio_dir\"], CONFIG[\"audio_extensions\"])\n",
        "    print_matching_report(match_result)\n",
        "\n",
        "    if not match_result[\"matched\"]:\n",
        "        print(\" No matched pairs found!\")\n",
        "        return\n",
        "\n",
        "    # ZONE 2: Extract segments\n",
        "    segments = extract_segments_from_matched(match_result[\"matched\"])\n",
        "\n",
        "    # ZONE 3: Segment audio\n",
        "    segmenter = AudioSegmenter(CONFIG[\"output_dir\"] / \"audio_segments\")\n",
        "    segment_all_result = segmenter.segment_all(segments, match_result[\"matched\"], max_segments=100)\n",
        "    audio_segments = segment_all_result[\"extracted\"]\n",
        "\n",
        "    # ZONE 4: Get speakers info\n",
        "    speakers_info = extract_all_speakers_info(match_result[\"matched\"])\n",
        "\n",
        "    # ZONE 5: Evaluate Whisper (children only)\n",
        "    evaluator = WhisperEvaluator(CONFIG[\"whisper_model\"])\n",
        "    results = evaluator.evaluate_children(audio_segments, speakers_info, CONFIG[\"sample_size\"])\n",
        "\n",
        "    print_wer_report(results, speakers_info)\n",
        "\n",
        "    # ZONE 6: Create dataset\n",
        "    builder = DatasetBuilder(CONFIG[\"output_dir\"] / \"training_dataset\")\n",
        "    builder.create_dataset(results, CONFIG[\"train_ratio\"])\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"PIPELINE COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nOutput directory: {CONFIG['output_dir']}\")\n",
        "    print(\"\\nFiles generated:\")\n",
        "    print(\"   training_dataset/train.jsonl ........... for fine-tuning\")\n",
        "    print(\"   training_dataset/eval.jsonl ........... for evaluation\")\n",
        "    print(\"   training_dataset/train_metadata.json .. complete metadata\")\n",
        "    print(\"   training_dataset/eval_metadata.json ... complete metadata\")\n",
        "    print(\"\\nReady for Whisper fine-tuning!\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5d5ac8b292fc833",
        "outputId": "2d6828dc-3aaa-4276-a0ce-b0f724b2ad24"
      },
      "id": "e5d5ac8b292fc833"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}